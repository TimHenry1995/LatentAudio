# LatentAudio
The goal of this work was to disentangle the latent space of sound-event recognition model [Yamnet](https://www.tensorflow.org/hub/tutorials/yamnet) into materials and actions. The data consists of 60K 1-second long sound snippets for which it is known which material and action was involved in making the sound. Yamnet is a 14 layer convolutional neural network that maps a sound's spectrogram to 521 common-sense auditory event classes. For detailed explanations of our experiments, refer to our award-winning [research paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4959687).

### Installation
If you have a Windows or Intel-based Mac computer, then you can run the installation on your local machine. To do so, download the Analysis.ipynb notebook from here, place it in a new folder on your computer and open it, e.g. in Visual Studio Code. Alternatively, you can open the notebook in the cloud computer of [Google Colab](https://colab.google). The notebook also runs you through the installation both locally and in the cloud. 

## Analysis
Researchers who are interested in verifying or adjusting the actual analysis should work with the analysis section of "Analysis.ipynb". It pre-processes sounds into their latent Yamnet representations, then takes these representations and passes them through PCA and t-SNE for visualization. It also uses KNN to classify them into materials and actions. Using our custom flow model library [Gyoza](https://pypi.org/project/gyoza/), it then disentangles two factors (material and action) of the latent space and visualizes the resulting 2D projections. The disentangled representations are then perturbed along the material or action dimensions and fed back through the inverse flow model and inverse PCA model to continue downstream Yamnet processing. Systematic changes in Yamnets output are demonstrated as a result. Numerous statistical tests and figures for these analyses are also provided. 

